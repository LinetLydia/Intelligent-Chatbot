{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40b4fa9-fda3-4769-a26a-43894f3e153b",
   "metadata": {},
   "source": [
    "# Intelligent Chatbot Development Project\n",
    "\n",
    "## Introduction\n",
    "This project presents the development of an **Intelligent Learning Chatbot**, designed to simulate human-like dialogue and assist users in exploring topics related to artificial intelligence, data science, and machine learning. The chatbot leverages **Natural Language Processing (NLP)** and **Machine Learning (ML)** techniques to interpret user intent, extract key information, and provide accurate, context-aware responses.\n",
    "\n",
    "Built using **Python**, **spaCy**, **Sentence Transformers**, and **Gradio**, the chatbot follows a **retrieval-based approach**, ensuring that every answer is grounded in existing data rather than generated arbitrarily. This approach guarantees both accuracy and control, allowing the chatbot to deliver consistent and meaningful interactions.\n",
    "\n",
    "The project emphasizes clarity, reproducibility, and modular design, with each phase, from dataset creation and embedding generation to model evaluation and deployment, structured for ease of understanding and future scalability. The chatbot is deployed on **Hugging Face Spaces**, enabling real-time interaction through a user-friendly interface.\n",
    "\n",
    "### Goals\n",
    "The primary objectives of this project are to:\n",
    "\n",
    "1. **Develop a reproducible NLP workflow** for loading, cleaning, and structuring conversational data.\n",
    "\n",
    "2. **Build a retrieval-based chatbot** capable of accurately identifying user intent and delivering the most relevant response.\n",
    "\n",
    "3. **Implement entity recognition and sentiment awareness** to enrich and personalize responses.\n",
    "\n",
    "4. **Maintain version control and collaboration** using Git and GitHub throughout the development process.\n",
    "\n",
    "5. **Deploy a web-based chatbot interface** using Gradio for live, interactive demonstrations.\n",
    "\n",
    "### Milestones\n",
    "**M1: Data Loading & Cleaning** â€“ Prepare and preprocess raw conversational and intent data.\n",
    "\n",
    "**M2: Embeddings & Retrieval** â€“ Generate text embeddings using Sentence Transformers and set up semantic search.\n",
    "\n",
    "**M3: Intent Classification (Baseline)** â€“ Establish a foundation for intent detection and response mapping.\n",
    "\n",
    "**M4: Entity & Sentiment Enhancement** â€“ Integrate named entity recognition and sentiment awareness for smarter replies.\n",
    "\n",
    "**M5: Interactive Chat UI (Gradio)** â€“ Develop and deploy an engaging user interface for real-time testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337094ee-d3ae-4283-b674-bc5c88b00467",
   "metadata": {},
   "source": [
    "### Step 1 - Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d010d676-5377-46c0-980b-9d43ce991e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet numpy pandas scikit-learn nltk spacy \"sentence-transformers>=3.0.0\" transformers gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7f5e8e-ea1f-493a-81d2-0ecbe093b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, sys\n",
    "# Downloading of small NLTK packs\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "import spacy\n",
    "try:\n",
    "    spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a577b89-f272-4980-819e-fc705e8bbee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\hp\\miniconda3\\lib\\site-packages (2.20.1)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\miniconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\miniconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\miniconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.7)\n",
      "Requirement already satisfied: pillow in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "580d4e6e-5f00-43bd-bcb5-d84339d9693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\miniconda3\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.16 | OS: Windows\n",
      "numpy                  -> 2.0.2\n",
      "pandas                 -> 2.2.3\n",
      "sentence-transformers  -> OK\n",
      "transformers           -> OK\n",
      "spacy                  -> 3.8.7\n",
      "nltk                   -> 3.9.2\n",
      "Embeddings shape: (2, 384)\n"
     ]
    }
   ],
   "source": [
    "import tf_keras as keras\n",
    "import sys, platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk, spacy\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n",
    "for name, mod in [\n",
    "    (\"numpy\", np), (\"pandas\", pd),\n",
    "    (\"sentence-transformers\", SentenceTransformer),\n",
    "    (\"transformers\", AutoTokenizer),\n",
    "    (\"spacy\", spacy), (\"nltk\", nltk),\n",
    "]:\n",
    "    try:\n",
    "        v = mod.__version__ if hasattr(mod, \"__version__\") else \"OK\"\n",
    "    except Exception:\n",
    "        v = \"OK\"\n",
    "    print(f\"{name:22s} -> {v}\")\n",
    "\n",
    "# Quick sanity check: load a tiny embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "sample = [\"hello world\", \"career in data science\"]\n",
    "emb = embedder.encode(sample, normalize_embeddings=True)\n",
    "print(\"Embeddings shape:\", emb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7633e8b-fb7a-4ea7-9c66-50b476fe9294",
   "metadata": {},
   "source": [
    "In Step 1, the project sets up the environment by installing and importing essential libraries used for Natural Language Processing (NLP), data handling, and model building. These include NumPy, Pandas, scikit-learn, NLTK, spaCy, Transformers, and Sentence Transformers. The setup ensures that all required NLP tools, such as tokenizers and stopwords, are downloaded, and the lightweight English model (en_core_web_sm) from spaCy is loaded for text cleaning and preprocessing. A small test is then performed using the Sentence Transformer model (all-MiniLM-L6-v2) to generate embeddings, numerical representations of text that capture meaning. This confirms that the libraries are correctly installed, the environment is configured, and the embedding model is functioning properly before moving to the next step of data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8fc9db-8186-41f6-8d3e-8bbf14238b53",
   "metadata": {},
   "source": [
    "### Step 2 - Data Setup (Test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1078a7d4-c94b-442c-b71c-4304d61f9bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>patterns</th>\n",
       "      <th>responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ask_skills_for_data_science</td>\n",
       "      <td>[What skills do I need for data science?, How ...</td>\n",
       "      <td>Data Science needs skills like Python, SQL, St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ask_skills_for_ai</td>\n",
       "      <td>[What do I need to study for AI?, What are AI ...</td>\n",
       "      <td>AI requires knowledge of Python, Neural Networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ask_job_recommendation</td>\n",
       "      <td>[What job is good for me if I like numbers?, I...</td>\n",
       "      <td>You might enjoy careers like Data Analyst, Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ask_learning_path</td>\n",
       "      <td>[Where should I start learning data analysis?,...</td>\n",
       "      <td>Start with Python, then learn data visualizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>greeting</td>\n",
       "      <td>[Hi, Hello, Hey]</td>\n",
       "      <td>Hello! How can I help you today?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>[Bye, See you later, Goodbye]</td>\n",
       "      <td>Goodbye! Keep learning and stay curious!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        intent  \\\n",
       "0  ask_skills_for_data_science   \n",
       "1            ask_skills_for_ai   \n",
       "2       ask_job_recommendation   \n",
       "3            ask_learning_path   \n",
       "4                     greeting   \n",
       "5                      goodbye   \n",
       "\n",
       "                                            patterns  \\\n",
       "0  [What skills do I need for data science?, How ...   \n",
       "1  [What do I need to study for AI?, What are AI ...   \n",
       "2  [What job is good for me if I like numbers?, I...   \n",
       "3  [Where should I start learning data analysis?,...   \n",
       "4                                   [Hi, Hello, Hey]   \n",
       "5                      [Bye, See you later, Goodbye]   \n",
       "\n",
       "                                           responses  \n",
       "0  Data Science needs skills like Python, SQL, St...  \n",
       "1  AI requires knowledge of Python, Neural Networ...  \n",
       "2  You might enjoy careers like Data Analyst, Sta...  \n",
       "3  Start with Python, then learn data visualizati...  \n",
       "4                   Hello! How can I help you today?  \n",
       "5           Goodbye! Keep learning and stay curious!  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"intent\": [\n",
    "        \"ask_skills_for_data_science\",\n",
    "        \"ask_skills_for_ai\",\n",
    "        \"ask_job_recommendation\",\n",
    "        \"ask_learning_path\",\n",
    "        \"greeting\",\n",
    "        \"goodbye\",\n",
    "    ],\n",
    "    \"patterns\": [\n",
    "        [\"What skills do I need for data science?\", \"How to become a data scientist?\"],\n",
    "        [\"What do I need to study for AI?\", \"What are AI skills?\"],\n",
    "        [\"What job is good for me if I like numbers?\", \"I enjoy problem-solving, any job ideas?\"],\n",
    "        [\"Where should I start learning data analysis?\", \"Best way to learn machine learning?\"],\n",
    "        [\"Hi\", \"Hello\", \"Hey\"],\n",
    "        [\"Bye\", \"See you later\", \"Goodbye\"],\n",
    "    ],\n",
    "    \"responses\": [\n",
    "        \"Data Science needs skills like Python, SQL, Statistics, and Machine Learning.\",\n",
    "        \"AI requires knowledge of Python, Neural Networks, and Data Modeling.\",\n",
    "        \"You might enjoy careers like Data Analyst, Statistician, or Research Scientist.\",\n",
    "        \"Start with Python, then learn data visualization and basic machine learning.\",\n",
    "        \"Hello! How can I help you today?\",\n",
    "        \"Goodbye! Keep learning and stay curious!\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcacc4e3-4738-4002-a5a7-7f849479aadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What skills do I need for data science?</td>\n",
       "      <td>ask_skills_for_data_science</td>\n",
       "      <td>Data Science needs skills like Python, SQL, St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to become a data scientist?</td>\n",
       "      <td>ask_skills_for_data_science</td>\n",
       "      <td>Data Science needs skills like Python, SQL, St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do I need to study for AI?</td>\n",
       "      <td>ask_skills_for_ai</td>\n",
       "      <td>AI requires knowledge of Python, Neural Networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are AI skills?</td>\n",
       "      <td>ask_skills_for_ai</td>\n",
       "      <td>AI requires knowledge of Python, Neural Networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What job is good for me if I like numbers?</td>\n",
       "      <td>ask_job_recommendation</td>\n",
       "      <td>You might enjoy careers like Data Analyst, Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I enjoy problem-solving, any job ideas?</td>\n",
       "      <td>ask_job_recommendation</td>\n",
       "      <td>You might enjoy careers like Data Analyst, Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Where should I start learning data analysis?</td>\n",
       "      <td>ask_learning_path</td>\n",
       "      <td>Start with Python, then learn data visualizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Best way to learn machine learning?</td>\n",
       "      <td>ask_learning_path</td>\n",
       "      <td>Start with Python, then learn data visualizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hi</td>\n",
       "      <td>greeting</td>\n",
       "      <td>Hello! How can I help you today?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hello</td>\n",
       "      <td>greeting</td>\n",
       "      <td>Hello! How can I help you today?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text                       intent  \\\n",
       "0       What skills do I need for data science?  ask_skills_for_data_science   \n",
       "1               How to become a data scientist?  ask_skills_for_data_science   \n",
       "2               What do I need to study for AI?            ask_skills_for_ai   \n",
       "3                           What are AI skills?            ask_skills_for_ai   \n",
       "4    What job is good for me if I like numbers?       ask_job_recommendation   \n",
       "5       I enjoy problem-solving, any job ideas?       ask_job_recommendation   \n",
       "6  Where should I start learning data analysis?            ask_learning_path   \n",
       "7           Best way to learn machine learning?            ask_learning_path   \n",
       "8                                            Hi                     greeting   \n",
       "9                                         Hello                     greeting   \n",
       "\n",
       "                                            response  \n",
       "0  Data Science needs skills like Python, SQL, St...  \n",
       "1  Data Science needs skills like Python, SQL, St...  \n",
       "2  AI requires knowledge of Python, Neural Networ...  \n",
       "3  AI requires knowledge of Python, Neural Networ...  \n",
       "4  You might enjoy careers like Data Analyst, Sta...  \n",
       "5  You might enjoy careers like Data Analyst, Sta...  \n",
       "6  Start with Python, then learn data visualizati...  \n",
       "7  Start with Python, then learn data visualizati...  \n",
       "8                   Hello! How can I help you today?  \n",
       "9                   Hello! How can I help you today?  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    for pattern in row[\"patterns\"]:\n",
    "        rows.append({\"text\": pattern, \"intent\": row[\"intent\"], \"response\": row[\"responses\"]})\n",
    "\n",
    "chatbot_df = pd.DataFrame(rows)\n",
    "chatbot_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0465e62-2136-4052-bca5-c10f00ad4d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: chatbot_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "chatbot_df.to_csv(\"chatbot_dataset.csv\", index=False)\n",
    "print(\"Saved: chatbot_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "593f1b97-b2d4-4e54-a684-8c4d56d77205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\hp\\miniconda3\\lib\\site-packages (1.7.4.5)\n",
      "Requirement already satisfied: bleach in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (3.4.1)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (6.33.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (75.8.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\hp\\miniconda3\\lib\\site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\miniconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39fc66e3-824f-4bba-a50f-814cd322879f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle config dir: C:\\Users\\hp\\projects\\intelligent-chatbot-notebook\\.kaggle\n",
      "Has kaggle.json: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = os.path.join(os.getcwd(), \".kaggle\")\n",
    "# Optional check:\n",
    "print(\"Kaggle config dir:\", os.environ[\"KAGGLE_CONFIG_DIR\"])\n",
    "print(\"Has kaggle.json:\", os.path.exists(os.path.join(os.environ[\"KAGGLE_CONFIG_DIR\"], \"kaggle.json\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f68b5105-0419-4d40-90ab-6030e14757dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/elvinagammed/chatbots-intent-recognition-dataset\n",
      "License(s): copyright-authors\n",
      "chatbots-intent-recognition-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d elvinagammed/chatbots-intent-recognition-dataset -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "384ba518-f855-47ed-8ee1-6d75795682cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files:\n",
      "data\\chatbots-intent-recognition-dataset.zip\n",
      "data\\Intent.json\n"
     ]
    }
   ],
   "source": [
    "import zipfile, glob\n",
    "\n",
    "zip_path = sorted(glob.glob(\"data/*.zip\"))[0]\n",
    "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "    z.extractall(\"data\")\n",
    "\n",
    "print(\"Extracted files:\")\n",
    "for p in glob.glob(\"data/**/*\", recursive=True):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f65be7-ca5e-4b37-83b5-0b2ce6eb261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intents']\n",
      "[{'intent': 'Greeting',\n",
      "  'responses': ['Hey there ğŸ‘‹ How are you doing today?',\n",
      "                'Hi! Great to see you ğŸ˜„',\n",
      "                'Hello, friend! Howâ€™s your day going?',\n",
      "                'Hey hey! What brings you here today?',\n",
      "                'Hi there! ğŸ˜Š What can I do for you?',\n",
      "                'Yo! ğŸ‘‹ Ready to chat?',\n",
      "                'Hey! Iâ€™m all ears â€” whatâ€™s on your mind?',\n",
      "                'Hello sunshine â˜€ï¸ How can I help?',\n",
      "                'Hi there! Iâ€™m your friendly AI assistant â€” what do you need?',\n",
      "                'Howdy partner ğŸ¤  Whatâ€™s up?'],\n",
      "  'text': ['Hi',\n",
      "           'Hey',\n",
      "           'Hey there',\n",
      "           'Hello',\n",
      "           'Hiya',\n",
      "           'Good morning',\n",
      "           'Good afternoon',\n",
      "           'Good evening',\n",
      "           \"What's up\",\n",
      "           'Yo',\n",
      "           'Howdy',\n",
      "           'Hello there',\n",
      "           'Hey bot',\n",
      "           'Hi friend',\n",
      "           'Greetings']},\n",
      " {'intent': 'Goodbye',\n",
      "  'responses': ['Goodbye ğŸ‘‹ Take care of yourself!',\n",
      "                'See you soon! ğŸ˜Š',\n",
      "                'Bye for now â€” come back anytime.',\n",
      "                'It was nice chatting with you!',\n",
      "                'Catch you later! ğŸ‘‹',\n",
      "                'Take care and stay awesome!',\n",
      "                'Iâ€™ll be here when you need me again.',\n",
      "                'See ya! Keep smiling ğŸ˜„'],\n",
      "  'text': ['Bye',\n",
      "           'Goodbye',\n",
      "           'See you later',\n",
      "           'Talk to you soon',\n",
      "           'Catch you later',\n",
      "           \"I'm leaving\",\n",
      "           'See ya',\n",
      "           'Later',\n",
      "           'Peace out',\n",
      "           'Take care']}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Step 1: Load and preview the JSON\n",
    "with open(\"Intent.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Step 2: Show the first few items so we can inspect structure\n",
    "if isinstance(data, dict):\n",
    "    # If it's a dictionary, show keys and sample content\n",
    "    pprint(list(data.keys()))\n",
    "    if \"intents\" in data:\n",
    "        pprint(data[\"intents\"][:2])\n",
    "    else:\n",
    "        pprint(data)\n",
    "else:\n",
    "    # If it's a list, show first 2 items\n",
    "    pprint(data[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e31f47b6-dfd6-4bda-85da-ad19e472ce97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 114 text patterns from 15 intents.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>text</th>\n",
       "      <th>responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Greeting</td>\n",
       "      <td>Hi</td>\n",
       "      <td>[Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Greeting</td>\n",
       "      <td>Hey</td>\n",
       "      <td>[Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Greeting</td>\n",
       "      <td>Hey there</td>\n",
       "      <td>[Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Greeting</td>\n",
       "      <td>Hello</td>\n",
       "      <td>[Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Greeting</td>\n",
       "      <td>Hiya</td>\n",
       "      <td>[Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     intent       text                                          responses\n",
       "0  Greeting         Hi  [Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...\n",
       "1  Greeting        Hey  [Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...\n",
       "2  Greeting  Hey there  [Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...\n",
       "3  Greeting      Hello  [Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...\n",
       "4  Greeting       Hiya  [Hey there ğŸ‘‹ How are you doing today?, Hi! Gre..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Convert JSON into a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "intents = data[\"intents\"]\n",
    "\n",
    "rows = []\n",
    "for intent in intents:\n",
    "    for pattern in intent[\"text\"]:\n",
    "        rows.append({\n",
    "            \"intent\": intent[\"intent\"],\n",
    "            \"text\": pattern,\n",
    "            \"responses\": intent[\"responses\"]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"âœ… Loaded {len(df)} text patterns from {len(intents)} intents.\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9609299-7502-445d-a37c-c6610423b3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Generating embeddings... please wait.\n",
      "âœ… Embeddings created and saved successfully as chatbot_with_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create embeddings and save chatbot data\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "print(\"ğŸ”„ Generating embeddings... please wait.\")\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# FIXED: use 'text' instead of 'pattern'\n",
    "df[\"embedding\"] = df[\"text\"].apply(lambda x: model.encode(x, normalize_embeddings=True))\n",
    "\n",
    "# Save updated data\n",
    "with open(\"chatbot_with_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "print(\"âœ… Embeddings created and saved successfully as chatbot_with_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6429105a-aec0-4fa3-a458-284494811c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings converted to NumPy arrays!\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert embeddings from list â†’ NumPy array\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.array(x))\n",
    "\n",
    "print(\"âœ… Embeddings converted to NumPy arrays!\")\n",
    "print(type(df[\"embedding\"].iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "392bb82e-c503-49cb-bc6c-3f978d7bf6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned dataset as chatbot_intents_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"chatbot_intents_dataset.csv\", index=False)\n",
    "print(\"Saved cleaned dataset as chatbot_intents_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03d643-e4a3-4e95-8ca8-16d0e6dad281",
   "metadata": {},
   "source": [
    "Next, a small sample dataset is created to simulate chatbot training data before using a larger real-world dataset. The dataset includes example user intents such as greetings, learning paths, and job recommendations, each paired with sample user messages (patterns) and chatbot replies (responses). This mock dataset helps verify that the data pipeline is working correctly. The project then saves the dataset as a CSV file (chatbot_dataset.csv) and connects to Kaggle using the Kaggle API to access and download a real dataset called â€œChatbots Intent Recognition Dataset.â€ The downloaded data, stored in a JSON file, is extracted and previewed to understand its structure and contents. Finally, the data is transformed into a clean, organized Pandas DataFrame containing three main columns, intent, text, and responses, and embeddings are generated using a Sentence Transformer model to prepare the dataset for chatbot training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe87f6e-2984-4968-a932-f60627051ef7",
   "metadata": {},
   "source": [
    "### Step 3 - Embeddings and Retrieval\n",
    "\n",
    "Weâ€™ll use a pre-trained SentenceTransformer model to create text embeddings for each pattern.\n",
    "These embeddings represent the meaning of text as numeric vectors.\n",
    "When a user sends a query, the chatbot will:\n",
    "1. Convert the query into an embedding,\n",
    "2. Compare it with all existing pattern embeddings using cosine similarity,\n",
    "3. Retrieve the most relevant response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea403c24-6ffa-4953-af27-bc5cfbbcdbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>text</th>\n",
       "      <th>responses</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Greeting</td>\n",
       "      <td>Hi</td>\n",
       "      <td>[Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...</td>\n",
       "      <td>[-0.09047622978687286, 0.04043959826231003, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Greeting</td>\n",
       "      <td>Hey</td>\n",
       "      <td>[Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...</td>\n",
       "      <td>[-0.11423862725496292, 0.013737436383962631, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Greeting</td>\n",
       "      <td>Hey there</td>\n",
       "      <td>[Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...</td>\n",
       "      <td>[-0.10038460791110992, 0.018592018634080887, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     intent       text                                          responses  \\\n",
       "0  Greeting         Hi  [Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...   \n",
       "1  Greeting        Hey  [Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...   \n",
       "2  Greeting  Hey there  [Hey there ğŸ‘‹ How are you doing today?, Hi! Gre...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.09047622978687286, 0.04043959826231003, 0....  \n",
       "1  [-0.11423862725496292, 0.013737436383962631, 0...  \n",
       "2  [-0.10038460791110992, 0.018592018634080887, 0...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = model.encode(df[\"text\"].tolist(), normalize_embeddings=True)\n",
    "\n",
    "df[\"embedding\"] = embeddings.tolist()\n",
    "\n",
    "# Sample\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a3cb97-aa8b-45e2-936c-3dfb58ff40ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_responses(user_input, df, model):\n",
    "    # Step 1: Encode user query\n",
    "    query_emb = model.encode([user_input], normalize_embeddings=True)\n",
    "    \n",
    "    # Step 2: Compute cosine similarity with existing embeddings\n",
    "    similarities = cosine_similarity(query_emb, np.vstack(df[\"embedding\"].values))[0]\n",
    "    \n",
    "    # Step 3: Pick the highest similarity\n",
    "    best_idx = np.argmax(similarities)\n",
    "    \n",
    "    # Step 4: Return intent, response, and similarity score\n",
    "    return {\n",
    "        \"user_input\": user_input,\n",
    "        \"predicted_intent\": df.iloc[best_idx][\"intent\"],\n",
    "        \"response\": df.iloc[best_idx][\"responses\"],\n",
    "        \"similarity\": round(similarities[best_idx], 3)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d8097dc-1bb9-447f-81f0-830903146221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ—£ï¸ User: hello there\n",
      "ğŸ¤– Bot (Greeting): ['Hey there ğŸ‘‹ How are you doing today?', 'Hi! Great to see you ğŸ˜„', 'Hello, friend! Howâ€™s your day going?', 'Hey hey! What brings you here today?', 'Hi there! ğŸ˜Š What can I do for you?', 'Yo! ğŸ‘‹ Ready to chat?', 'Hey! Iâ€™m all ears â€” whatâ€™s on your mind?', 'Hello sunshine â˜€ï¸ How can I help?', 'Hi there! Iâ€™m your friendly AI assistant â€” what do you need?', 'Howdy partner ğŸ¤  Whatâ€™s up?']\n",
      "Similarity: 1.0\n",
      "\n",
      "ğŸ—£ï¸ User: who are you\n",
      "ğŸ¤– Bot (Identity): ['Iâ€™m your intelligent chatbot assistant â€” created by Linet Lydia, the brilliant mind behind my design.', 'You can call me your digital companion ğŸ¤–. I was built by Linet Lydia!', 'Iâ€™m a blend of data, empathy, and code â€” created by Linet Lydia ğŸ’«.', 'Linet Lydia built me with the goal of making tech feel more human.', 'Iâ€™m an AI assistant designed to chat, help, and make you think!', 'Not human, but Iâ€™m learning from you every day ğŸ˜„', 'You could say Iâ€™m Linet Lydiaâ€™s digital creation â€” made with purpose and curiosity!']\n",
      "Similarity: 1.0\n",
      "\n",
      "ğŸ—£ï¸ User: bye\n",
      "ğŸ¤– Bot (Goodbye): ['Goodbye ğŸ‘‹ Take care of yourself!', 'See you soon! ğŸ˜Š', 'Bye for now â€” come back anytime.', 'It was nice chatting with you!', 'Catch you later! ğŸ‘‹', 'Take care and stay awesome!', 'Iâ€™ll be here when you need me again.', 'See ya! Keep smiling ğŸ˜„']\n",
      "Similarity: 1.0\n",
      "\n",
      "ğŸ—£ï¸ User: what can you do\n",
      "ğŸ¤– Bot (Identity): ['Iâ€™m your intelligent chatbot assistant â€” created by Linet Lydia, the brilliant mind behind my design.', 'You can call me your digital companion ğŸ¤–. I was built by Linet Lydia!', 'Iâ€™m a blend of data, empathy, and code â€” created by Linet Lydia ğŸ’«.', 'Linet Lydia built me with the goal of making tech feel more human.', 'Iâ€™m an AI assistant designed to chat, help, and make you think!', 'Not human, but Iâ€™m learning from you every day ğŸ˜„', 'You could say Iâ€™m Linet Lydiaâ€™s digital creation â€” made with purpose and curiosity!']\n",
      "Similarity: 1.0\n",
      "\n",
      "ğŸ—£ï¸ User: hi human\n",
      "ğŸ¤– Bot (Greeting): ['Hey there ğŸ‘‹ How are you doing today?', 'Hi! Great to see you ğŸ˜„', 'Hello, friend! Howâ€™s your day going?', 'Hey hey! What brings you here today?', 'Hi there! ğŸ˜Š What can I do for you?', 'Yo! ğŸ‘‹ Ready to chat?', 'Hey! Iâ€™m all ears â€” whatâ€™s on your mind?', 'Hello sunshine â˜€ï¸ How can I help?', 'Hi there! Iâ€™m your friendly AI assistant â€” what do you need?', 'Howdy partner ğŸ¤  Whatâ€™s up?']\n",
      "Similarity: 0.659\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"hello there\",\n",
    "    \"who are you\",\n",
    "    \"bye\",\n",
    "    \"what can you do\",\n",
    "    \"hi human\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    result = get_responses(q, df, model)\n",
    "    print(f\"\\nğŸ—£ï¸ User: {result['user_input']}\")\n",
    "    print(f\"ğŸ¤– Bot ({result['predicted_intent']}): {result['response']}\")\n",
    "    print(f\"Similarity: {result['similarity']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a55a39d4-ed03-4b4e-b92d-5f8019e83c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset with embeddings.\n"
     ]
    }
   ],
   "source": [
    "df.to_pickle(\"chatbot_with_embeddings.pkl\")\n",
    "print(\"Saved dataset with embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083b01b-111a-4303-9084-a65745e9068a",
   "metadata": {},
   "source": [
    "In Step 3, the chatbot begins learning how to understand and respond to user input through text embeddings. Using a pre-trained SentenceTransformer model (all-MiniLM-L6-v2), each question or pattern in the dataset is converted into a numerical vector that captures the meaning of the sentence. When a user enters a message, the chatbot transforms it into an embedding and compares it with all existing embeddings using cosine similarity to measure how closely they match. The response linked to the most similar pattern is then selected and displayed to the user. This step effectively enables the chatbot to understand natural language queries and respond intelligently, even if the exact phrasing wasnâ€™t part of the training data. The dataset, now containing the generated embeddings, is saved as a .pkl file for later use in the final chatbot interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c4409-8824-441e-846f-20bddc89d1b8",
   "metadata": {},
   "source": [
    "### Step 4 - Import Gradio and Build the Chat UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fc3c121-3d0d-4e01-8d9e-1ce81a485da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\hp\\miniconda3\\lib\\site-packages (5.49.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (4.8.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.120.3)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.6.4)\n",
      "Requirement already satisfied: gradio-client==1.13.3 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (1.13.3)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.36.0)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (3.1.5)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (2.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (3.11.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (2.10.6)\n",
      "Requirement already satisfied: pydub in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.14.3)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.49.1)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio-client==1.13.3->gradio) (2024.12.0)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.17.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.27.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\miniconda3\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\miniconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e57ab-8554-483f-ab83-093a2f8a7d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\miniconda3\\lib\\site-packages\\gradio\\chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://5894c53facce2b05e2.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5894c53facce2b05e2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def chatbot_reply(user_input):\n",
    "    if not user_input.strip():\n",
    "        return \"ğŸ¤– Please say something!\"\n",
    "\n",
    "    query_emb = model.encode([user_input], normalize_embeddings=True)\n",
    "    similarities = cosine_similarity(query_emb, np.vstack(df[\"embedding\"].values))[0]\n",
    "    best_idx = np.argmax(similarities)\n",
    "\n",
    "    intent = df.iloc[best_idx][\"intent\"]\n",
    "    responses = df.iloc[best_idx][\"responses\"]\n",
    "\n",
    "    # âœ… Choose one random response instead of printing the full list\n",
    "    if isinstance(responses, (list, tuple)) and responses:\n",
    "        response = random.choice(responses)\n",
    "    else:\n",
    "        response = str(responses)\n",
    "\n",
    "    score = round(similarities[best_idx], 3)\n",
    "    return f\"({intent}) â€” {response}\\n\\nConfidence: {score}\"\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=lambda message, history: chatbot_reply(message),\n",
    "    title=\"ğŸ§  Intelligent Chatbot\",\n",
    "    description=\"A chatbot that understands user intent using embeddings and retrieval.\",\n",
    ")\n",
    "\n",
    "demo.launch(share=True, debug=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4d77ae5-25ef-47d1-956e-c64b27842c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final chatbot saved successfully!\n"
     ]
    }
   ],
   "source": [
    "df.to_pickle(\"chatbot_with_embeddings.pkl\")\n",
    "print(\"Final chatbot saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a333ae1e-cfb7-40f5-967a-28a2ff8e44a7",
   "metadata": {},
   "source": [
    "In Step 4, the project integrates Gradio, a Python library that allows developers to create interactive web-based interfaces for machine learning models. After installing and importing Gradio, a simple chat interface is built to connect the backend logic with a user-friendly frontend. The chatbot function (chatbot_reply) processes the userâ€™s input by converting it into embeddings, calculating similarity scores, and selecting the most relevant response from the dataset. The gr.ChatInterface() is then used to display the conversation in real time, complete with message bubbles, predicted intent, and confidence levels. Finally, the chatbot is launched locally and deployed publicly through Hugging Face Spaces, allowing anyone to chat with the intelligent assistant directly from a browser. The chatbot file with all embeddings and updates is then saved to finalize the build."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab6275-5b41-4e5b-a82d-134552ea24f5",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4e60f-036e-4af8-b0cd-da1c21af00e2",
   "metadata": {},
   "source": [
    "This project successfully developed an intelligent chatbot capable of understanding and responding to user input using natural language processing and machine learning. Through a step-by-step approach, the system handled data preprocessing, intent classification, embedding generation, and interactive deployment. Using SentenceTransformers for semantic understanding and cosine similarity for response retrieval, the chatbot can interpret queries even when phrased differently from its training data. The integration of Gradio provided an intuitive, web-based interface for real-time interaction, while deployment on Hugging Face Spaces made the chatbot easily accessible online.\n",
    "\n",
    "Overall, this project demonstrates the complete lifecycle of building an AI-powered conversational assistant, from dataset creation and text processing to model deployment. Future improvements could include adding Named Entity Recognition (NER), sentiment analysis, or context tracking to make conversations more human-like and dynamic. The outcome highlights how modern NLP tools can be combined to create practical, intelligent systems for education, customer support, and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185fc5b-a506-41ba-b46d-89b6d29b7e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
